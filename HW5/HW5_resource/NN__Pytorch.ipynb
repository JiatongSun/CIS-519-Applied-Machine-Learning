{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN & Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cuf5GWGPNTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing NumPy and PyTorch related packages\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.datasets\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV2ixmgHTyd8",
        "colab_type": "text"
      },
      "source": [
        "#PyTorch Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alC1yV6IUOb1",
        "colab_type": "text"
      },
      "source": [
        "Before we go over the basics of creating a neural network, we need to get to know the language in which we are going to use to describe our neural network. We are using the PyTorch package to create our NN in Python. What is PyTorch you may ask? It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
        "\n",
        "*   A replacement for NumPy to use the power of GPUs\n",
        "*   a deep learning research platform that provides maximum flexibility and speed\n",
        "\n",
        "Since most of Neural Network's computation is massive matrix multiplications (both for forward and backward directions), we can parallelize the process heavily (thus speed things up significantly) by using the power of the GPU. This is where PyTorch has made it very simple for us to do! We don't need to directly write CUDA (parallel computing platform) kernels in C/C++ to do so, we can just write simple commands in Python and get roughly the same speed up benefits. Before we get started, lets make sure that you have enabled COLAB's GPU Setting. **It is so simple to alter default hardware (CPU to GPU or vice versa); just follow Edit > Notebook settings or Runtime>Change runtime type and select GPU as Hardware accelerator.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4OyWytVebP3",
        "colab_type": "code",
        "outputId": "5f4d1c4b-c9e5-4b31-d4cb-ed48491e0645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Now lets define our GPU instance (we use this throughout the code):\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um5F7mmQcPGt",
        "colab_type": "text"
      },
      "source": [
        "Tensor is the datatype that we work with in pytorch. Tensors are similar to NumPy’s ndarrays and you can create a randomly initilalzied one by following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP3f2F6YS6h9",
        "colab_type": "code",
        "outputId": "3609896a-9024-481a-f6f6-ba095ff12ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0785, 0.6236, 0.6687],\n",
            "        [0.9932, 0.5677, 0.1199],\n",
            "        [0.5778, 0.9978, 0.1031],\n",
            "        [0.0055, 0.3063, 0.5250],\n",
            "        [0.4225, 0.0433, 0.6711]])\n",
            "torch.Size([5, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE-6HIlKdHs9",
        "colab_type": "text"
      },
      "source": [
        "The tensor that you just created is on the CPU, but for you to be able to do operations on it using the GPU, you have to move it to the GPU memory by using the following command. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M34Irx8bdGjO",
        "colab_type": "code",
        "outputId": "3a903fbd-5c1d-4777-8fdb-dc67b1f40c15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "x = x.to(device)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0785, 0.6236, 0.6687],\n",
            "        [0.9932, 0.5677, 0.1199],\n",
            "        [0.5778, 0.9978, 0.1031],\n",
            "        [0.0055, 0.3063, 0.5250],\n",
            "        [0.4225, 0.0433, 0.6711]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlt3SDMPfbGV",
        "colab_type": "text"
      },
      "source": [
        "We can convert a Torch Tensor to a NumPy array and vice versa easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i20Dfxl-fapK",
        "colab_type": "code",
        "outputId": "26323433-328d-438b-b3a1-2d8a0606ea55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "x = torch.rand(5, 3) # Tensor on the CPU\n",
        "x_numpy = x.numpy()\n",
        "print('Numpy array: ', x_numpy)\n",
        "x_tensor= torch.from_numpy(x_numpy)\n",
        "print('Torch Tensor: ', x_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numpy array:  [[0.45583892 0.42274487 0.2455194 ]\n",
            " [0.9713313  0.08420366 0.35444242]\n",
            " [0.40629393 0.70475537 0.03286195]\n",
            " [0.9826768  0.5062513  0.05177331]\n",
            " [0.9118254  0.4811377  0.38271707]]\n",
            "Torch Tensor:  tensor([[0.4558, 0.4227, 0.2455],\n",
            "        [0.9713, 0.0842, 0.3544],\n",
            "        [0.4063, 0.7048, 0.0329],\n",
            "        [0.9827, 0.5063, 0.0518],\n",
            "        [0.9118, 0.4811, 0.3827]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCg_BbIMgfl6",
        "colab_type": "text"
      },
      "source": [
        "To learn more about the basics of PyTorch please look at the tutorials in the link below:\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpwkDqqwhT8p",
        "colab_type": "text"
      },
      "source": [
        "#Create a Neural Net on Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRqJLjDGj-Qw",
        "colab_type": "text"
      },
      "source": [
        "First thing first, to create a network we need to create a class that uses nn.Linear to create linear layers for our NN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzNgyhK_gZoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.LLayer1 = nn.Linear(24, 12, bias = True) ## Linear layer that goes from 24 features to 12 features\n",
        "        self.LLayer2 = nn.Linear(12, 5, bias = True) ## Linear layer that goes from 12 features to 5 features\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):  ## We need to create a method that takes care of the forward pass of the NN. The backward pass is automatically taken care of so\n",
        "                           ## you don't need to worry about that!\n",
        "        Layer1_out = self.LLayer1(x)\n",
        "        Layer2_out = self.LLayer2(Layer1_out)\n",
        "        output = self.sigmoid(Layer2_out)\n",
        "        \n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dco2-KgFmbaX",
        "colab_type": "text"
      },
      "source": [
        "We just created Neural Network Structure that looks like the following:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/KfjibwT.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFLCSVNoiap",
        "colab_type": "text"
      },
      "source": [
        "We initialize our model in PyTroch (with random weights) by creating an instance of the model class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzMwzeaJoVru",
        "colab_type": "code",
        "outputId": "0ee324e6-da7e-4343-c38e-01cf357c09dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "model = Network() # Initializes the model on the CPU\n",
        "\n",
        "model = model.to(device) # We can move the model (weights) to the GPU memory by calling this\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (LLayer1): Linear(in_features=24, out_features=12, bias=True)\n",
              "  (LLayer2): Linear(in_features=12, out_features=5, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eGt5__Il3M_",
        "colab_type": "text"
      },
      "source": [
        "We also need to define a loss function to then use during learning. let's say we go with cross entrpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cNY7e_Il2fB",
        "colab_type": "code",
        "outputId": "5d19487f-da34-490e-f983-3fee0be08fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss_func = nn.CrossEntropyLoss() \n",
        "loss_func = loss_func.to(device)\n",
        "loss_func"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBi_hwzsl23b",
        "colab_type": "text"
      },
      "source": [
        "And we need to define an optimizer that does the optimization (backprop process) using the loss_func and output of our model. Let's go with the familiar SGD that you have worked with in previous assigments and worksheets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gAqmnNRpmG7",
        "colab_type": "code",
        "outputId": "580f0bcc-6a2e-4664-b563-d35a9cc91af1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    lr: 0.01\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_LT2qHGpyo6",
        "colab_type": "text"
      },
      "source": [
        "Now we have successfully initialized our NN using Pytorch on Python! In the next worksheet, we will go through an image classification example that shows you how to use all these elements together to successfully make your very own image classifier! Stay tuned! In the meanwhile, I suggest you go over the slides and the PyTorch documentation (especially the link provided above), to understand the theory behind NNs and to learn how to apply your NN knowledge in building useful applications using PyTorch."
      ]
    }
  ]
}