\documentclass{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage{empheq}

\title{CIS 419/519: Homework 5}
\author{Jiatong Sun}
\date{03/13/2020}

\begin{document}
    \maketitle
    \noindent
    Although the solutions are entirely my own, I consulted with the following people and sources while working on this homework: $Yuchen Sun$, $Junfan Pan$\\
    https://stackoverflow.com/questions/50994504/how-to-put-figure-between-items-on-enumerate-list
    
    \section{Logical Functions with Neural Nets}
        \begin{enumerate}[label=\alph*.]
            \item % a
				\begin{minipage}[t]{\linewidth}
                	\captionsetup{type=figure}
                	\centering
                	\includegraphics[width=0.6\linewidth]
                					{images/Q1a.jpg}
                	\caption{NAND}      
        		\end{minipage} 
        		
        		
        		\begin{table}[h]
        			\centering
					\begin{tabularx}{0.8\textwidth} { 
 						| >{\centering\arraybackslash}X 
  						| >{\centering\arraybackslash}X 
   						| >{\centering\arraybackslash}X | }
   						\hline
   						\multicolumn{3}{|c|}
   						{\textbf{Truth Table}}\\
 						\hline
 					 	$x_1$ & $x_2$ & $H(x)$ \\
 						\hline
 						0 & 0 & $\sigma(30)=1$\\
 						\hline
 						0 & 1 & $\sigma(10)=1$\\
 						\hline
 						1 & 0 & $\sigma(10)=1$\\
 						\hline
 						1 & 1 & $\sigma(-10)=0$\\
						\hline
					\end{tabularx} 
					\caption{NAND}
					\label{tab:1}
				\end{table}	        		           
            
            \item % b
            	\begin{minipage}[t]{\linewidth}
                	\captionsetup{type=figure}
                	\centering
                	\includegraphics[width=0.6\linewidth]
                					{images/Q1b.jpg}
                	\caption{NAND}      
        		\end{minipage} 
        		
        		
        		\begin{table}[h]
        			\centering
					\begin{tabularx}{0.8\textwidth} { 
 						| >{\centering\arraybackslash}X 
  						| >{\centering\arraybackslash}X 
  						| >{\centering\arraybackslash}X 
  						| >{\centering\arraybackslash}X
  						| >{\centering\arraybackslash}X
  						| >{\centering\arraybackslash}X
  						| >{\centering\arraybackslash}X
  						| >{\centering\arraybackslash}X
   						| >{\centering\arraybackslash}X | }
   						\hline
   						\multicolumn{9}{|c|}
   						{\textbf{Truth Table}}\\
 						\hline
 					 	$w_{10}$ & $w_{11}$ & $w_{12}$ & $w_{13}$ &
 					 	$w_{20}$ & $w_{21}$ & $w_{22}$ & $w_{23}$ & 
 					 	$w_{24}$ \\
 						\hline
 						-30 & 20 & 20 & 20 & -10 & 20 & 20 & 20 & -40\\
 						\hline
					\end{tabularx} 
					\caption{Weight table}
					\label{tab:2}
				\end{table}	
        		
        		
        		\begin{table}[h]
        			\centering
					\begin{tabularx}{0.8\textwidth} { 
 						| >{\centering\arraybackslash}X 
  						| >{\centering\arraybackslash}X 
  						| >{\centering\arraybackslash}X 
  						| >{\centering\arraybackslash}X
   						| >{\centering\arraybackslash}X | }
   						\hline
   						\multicolumn{5}{|c|}
   						{\textbf{Truth Table}}\\
 						\hline
 					 	$x_1$ & $x_2$ & $x_3$ & $y_1$ & $H(x)$ \\
 						\hline
 						0 & 0 & 0 & $\sigma(-30)=0$ & $\sigma(-10)=0$\\
 						\hline
 						0 & 0 & 1 & $\sigma(-10)=0$ & $\sigma(10)=1$\\
 						\hline
 						0 & 1 & 0 & $\sigma(-10)=0$ & $\sigma(10)=1$\\
 						\hline
 						0 & 1 & 1 & $\sigma(-10)=0$ & $\sigma(10)=1$\\
 						\hline
 						1 & 0 & 0 & $\sigma(10)=1$ & $\sigma(-10)=0$\\
 						\hline
 						1 & 0 & 1 & $\sigma(10)=1$ & $\sigma(-10)=0$\\
 						\hline
 						1 & 1 & 0 & $\sigma(10)=1$ & $\sigma(-10)=0$\\
 						\hline
 						1 & 1 & 1 & $\sigma(30)=1$ & $\sigma(10)=1$\\
						\hline
					\end{tabularx} 
					\caption{Parity}
					\label{tab:3}
				\end{table}	  
            
        \end{enumerate}
        
    \section{Calculating Backprop by Hand}
    	In the following solution, superscript $l$ denotes the number of layer and subscript $p$ denotes the number's position in its matrix. \\\\
    	$z^l_p$ denotes the dot product of weights and last layer's output, and $a^l_p$ denotes the output of current layer, which is after the activation function.\\\\
    	Since the hidden layer uses a sign function and the output layer uses a sigmoid function, the output can be expressed as follows:
    	$$a^2=
    	Sigmoid(z^2)=
    	\dfrac{1}{1+e^{-z^2}}=
    	\dfrac{1}{1+e^{-\boldsymbol{W^2}a^1}}=
    	\dfrac{1}{1+e^{-\boldsymbol{W^2}\cdot Sign(\boldsymbol{W^1}\boldsymbol{X})}}$$
    	where
    	$$a^1=
    	Sign(z_1)=
    	Sign(\boldsymbol{W^1}\boldsymbol{X})=
    	Sign(\begin{bmatrix}0.1 & 0.2\\-0.4 & 0.3\end{bmatrix} \begin{bmatrix}5\\4\end{bmatrix})=
    	Sign(\begin{bmatrix}1.3\\-0.8\end{bmatrix})=
    	\begin{bmatrix}1\\-1\end{bmatrix}$$
    	and
    	$$a^2=
    	\dfrac{1}{1+e^{-\boldsymbol{W^2}a^1}}=
    	\dfrac{1}{1+e^{-\begin{bmatrix}0.1&0.2\end{bmatrix}\begin{bmatrix}1\\-1\end{bmatrix}}}=
    	\dfrac{1}{1+e^{0.1}}=0.475$$
    	To calculate the output gradient with respect to each of the weights, we can use the chain rule. Every part in the partial derivative chain is calculated and listed in:\\
    	\begin{minipage}[t]{\linewidth}
        		\captionsetup{type=figure}
               	\centering
               	\includegraphics[width=0.6\linewidth]
                				{images/Q2a.jpg}
                \caption{Forward Propagation}      
        \end{minipage}
		$$$$
        \begin{minipage}[t]{\linewidth}
        		\captionsetup{type=figure}
               	\centering
               	\includegraphics[width=0.6\linewidth]
                				{images/Q2b.jpg}
                \caption{Backprpogation}      
        \end{minipage} 
        
		\newpage        
		\[
  			\dfrac{da^2}{d\boldsymbol{W^2}}:
  			\begin{cases}
               \dfrac{da^2}{\partial{w^2_{21}}}=\dfrac{\partial a^2}{\partial{z^2}}\dfrac{\partial z^2}{\partial{w^2_1}}=0.249\cdot1=0.249\\\\
               \dfrac{da^2}{\partial{w^2_{21}}}=\dfrac{\partial a^2}{\partial{z^2}}\dfrac{\partial z^2}{\partial{w^2_2}}=0.249\cdot(-1)=-0.249
            \end{cases}
		\]
		$$$$
		\[
  			\dfrac{da^2}{d\boldsymbol{W^1}}:
  			\begin{cases}
               \dfrac{da^2}{\partial{w^1_{11}}}=\dfrac{\partial a^2}{\partial{z^2}}\dfrac{\partial z^2}{\partial{a^1_1}}\dfrac{\partial a^1_1}{\partial{z^1_1}}\dfrac{\partial z^1_1}{\partial{w^1_{11}}}=0.249\cdot0.1\cdot0\cdot5=0\\\\
               \dfrac{da^2}{\partial{w^1_{12}}}=\dfrac{\partial a^2}{\partial{z^2}}\dfrac{\partial z^2}{\partial{a^1_1}}\dfrac{\partial a^1_1}{\partial{z^1_1}}\dfrac{\partial z^1_1}{\partial{w^1_{12}}}=0.249\cdot0.1\cdot0\cdot4=0\\\\
               \dfrac{da^2}{\partial{w^1_{21}}}=\dfrac{\partial a^2}{\partial{z^2}}\dfrac{\partial z^2}{\partial{a^1_2}}\dfrac{\partial a^1_2}{\partial{z^1_2}}\dfrac{\partial z^1_2}{\partial{w^1_{21}}}=0.249\cdot0.2\cdot1\cdot5=0.249\\\\
               \dfrac{da^2}{\partial{w^1_{22}}}=\dfrac{\partial a^2}{\partial{z^2}}\dfrac{\partial z^2}{\partial{a^1_2}}\dfrac{\partial a^1_2}{\partial{z^1_2}}\dfrac{\partial z^1_2}{\partial{w^1_{22}}}=0.249\cdot0.2\cdot1\cdot4=0.199
            \end{cases}
		\]
		
    	So we finally get
    	
    	$$\dfrac{da^2}{d\boldsymbol{W^2}}=\begin{bmatrix}0.249\\-0.249\end{bmatrix},\quad \dfrac{da^2}{d\boldsymbol{W^1}}=\begin{bmatrix}0 & 0\\0.249 & 0.199\end{bmatrix}$$
    	$$$$
			
        
    \section{Neural Nets in SuperTuxKart}
       
\end{document}