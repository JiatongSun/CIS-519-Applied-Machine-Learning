\documentclass{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage{empheq}
\usepackage{stackengine}

\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}


\title{CIS 419/519: Homework 6}
\author{Jiatong Sun}
\date{04/11/2020}

\begin{document}
    \maketitle
    \noindent
    Although the solutions are entirely my own, I consulted with the following people and sources while working on this homework: 
    
    \section{Reinforcement Learning \uppercase\expandafter{\romannumeral 1}}
    \noindent
    The reward function is fine because for a situation to have a robot run a maze can break down naturally into episodes and thus, simply using the sum of rewards is reasonable.\\\\
    However, if the map is large and the episode length is not long enough, the agent will probably fail to discover the exit for many episodes. After running the agent for a while, the agent may never know there can be a total reward better than 0. Besides, since every policy it has already tried gives the same total reward 0, the agent will have no clue how to choose its policy for the following episode. Therefore, the policy is not improved.\\\\
    There are two easy ways to solve the problem: increase the episode length and/or give non-goal state a value -1. This means the states that have been visited a lot will get worse and worse values so the agent will want to avoid that state. In this way, the agent can eventually reach the goal.
        
    \section{Reinforcement Learning \uppercase\expandafter{\romannumeral 2}}
    According to equation for the expected discount return and value of a state:
    $$R_t=\sum_{k=0}^{\infty}\gamma^kr_{t+k+1}\qquad V^{\pi}(s)=\mathbb{E}[R_t|s_t=s]$$
    If we add a constant $C$ to all the rewards, the total reward changes into
    \begin{equation}\label{eq:reward}
    	\begin{split}
    	R'_t
    	&=\sum_{k=0}^{\infty}\gamma^k(r_{t+k+1}+C)\\
    	&=\sum_{k=0}^{\infty}\gamma^kr_{t+k+1}+\sum_{k=0}^{\infty}\gamma^kC\\
    	&=\sum_{k=0}^{\infty}\gamma^kr_{t+k+1}+\dfrac{C}{1-\gamma} \\
    	&=R_t+\dfrac{C}{1-\gamma}
    	\end{split}
    \end{equation}
    The state value changes into
    \begin{equation}\label{eq:state value}
    	\begin{split}
    	V^{\pi'}(s)
    	&=\mathbb{E}[R_t'|s_t=s]\\
    	&=\mathbb{E}[R_t+\dfrac{C}{1-\gamma}|s_t=s]\\
    	&=\mathbb{E}[R_t|s_t=s]+\dfrac{C}{1-\gamma}\\
    	&=V^{\pi}(s)+\dfrac{C}{1-\gamma}
    	\end{split}
    \end{equation}
    Define $K\delequal\sum_{k=0}^{\infty}\gamma^kC=\dfrac{C}{1-\gamma}$, then $K$ is a constant.\\\\
    \begin{enumerate}
    	\item[(a)]
    	From equation \ref{eq:reward} and equation \ref{eq:state value}, we know that adding a constant to all the rewards only has the effect of shifting the total reward and state value, so the relative reward and relative state value remain the same. \\\\
    	Because we choose the policy that maximize the total reward, we still get the same best policy as before. So \textbf{only the interval between those rewards matters}.\\\\
    	The signs of those rewards are not important. For example, if we change \{+1 for a goal, -1 for a collision\} to \{-1 for a goal, -3 for a collision\}, the best policy gets worse rewards, but it will still be the best policy among all. Changing every reward to negative value essentially means finding a relative better policy among a group of bad policies.
    	\item[(b)]See the proof above.
    	\item[(c)]$K=\sum_{k=0}^{\infty}\gamma^kC=\dfrac{C}{1-\gamma},\; 0<\gamma<1$
    \end{enumerate}
    
    	
    \section{Random policy for the MountainCar gym environment}
       
\end{document}