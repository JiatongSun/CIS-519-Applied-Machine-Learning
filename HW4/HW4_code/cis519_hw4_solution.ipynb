{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIO3UIZe6wsZ"
   },
   "source": [
    "# CIS 419/519 \n",
    "#**Homework 4 : Adaboost and the Challenge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gS022EH9_-p"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjPfIJ5G52It"
   },
   "source": [
    "# Adaboost-SAMME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdiCAgcNIt_m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn import tree\n",
    "\n",
    "class BoostedDT:\n",
    "\n",
    "    def __init__(self, numBoostingIters=100, maxTreeDepth=3):\n",
    "        '''\n",
    "        Constructor\n",
    "\n",
    "        Class Fields \n",
    "        clfs : List object containing individual DecisionTree classifiers, in order of creation during boosting\n",
    "        betas : List of beta values, in order of creation during boosting\n",
    "        '''\n",
    "\n",
    "        self.clfs = []  # keep the class fields, and be sure to keep them updated during boosting\n",
    "        self.betas = []\n",
    "        self.numBoostingIters = numBoostingIters\n",
    "        self.maxTreeDepth = maxTreeDepth\n",
    "        self.K = None\n",
    "        self.classes = None\n",
    "        \n",
    "        #TODO\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, random_state=None):\n",
    "        '''\n",
    "        Trains the model. \n",
    "        Be sure to initialize all individual Decision trees with the provided random_state value if provided.\n",
    "        \n",
    "        Arguments:\n",
    "            X is an n-by-d Pandas Data Frame\n",
    "            y is an n-by-1 Pandas Data Frame\n",
    "            random_seed is an optional integer value\n",
    "        '''\n",
    "        #TODO\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "        n,d = X.shape\n",
    "        y = y.reshape(-1,1)\n",
    "        \n",
    "        self.classes = np.unique(y)\n",
    "        self.K = len(self.classes)\n",
    "        \n",
    "        weights = np.full(n,1/n).reshape(-1,1)\n",
    "        \n",
    "        for iter_num in range(self.numBoostingIters):\n",
    "            h = tree.DecisionTreeClassifier(max_depth = self.maxTreeDepth,\n",
    "                                              random_state = random_state)\n",
    "            h.fit(X,y,sample_weight = weights.flatten())\n",
    "            self.clfs.append(h)\n",
    "            y_pred = h.predict(X).reshape(-1,1)\n",
    "            epsilon = np.sum((y_pred!=y)*weights)\n",
    "            beta = np.log((self.K-1)*(1-epsilon)/epsilon)/2\n",
    "            self.betas.append(beta)\n",
    "            weights[y_pred==y] *= np.exp(-beta)\n",
    "            weights[y_pred!=y] *= np.exp(beta)\n",
    "            weights /= sum(weights)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is an n-by-d Pandas Data Frame\n",
    "        Returns:\n",
    "            an n-by-1 Pandas Data Frame of the predictions\n",
    "        '''\n",
    "        #TODO\n",
    "        X = X.to_numpy()\n",
    "        n,d = X.shape\n",
    "        proba = np.zeros((n,self.K))\n",
    "        for iter_num in range(self.numBoostingIters):\n",
    "            proba += self.clfs[iter_num].predict_proba(X)\n",
    "        max_proba = np.argmax(proba,axis=1).reshape(-1)\n",
    "        pred_array = np.tile(self.classes,(n,1))\n",
    "        y_pred = np.choose(max_proba,pred_array.T).reshape(-1,1)\n",
    "        return pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DAfYDnGU9l8"
   },
   "source": [
    "# Test BoostedDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxDOJf2rIr2Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy = 0.9263157894736842\n",
      "My Boosted Decision Tree Accuracy = 0.9578947368421052\n",
      "Sklearn's Boosted Decision Tree Accuracy = 0.9543859649122807\n",
      "\n",
      "Note that due to randomization, your boostedDT might not always have the \n",
      "exact same accuracy as Sklearn's boostedDT.  But, on repeated runs, they \n",
      "should be roughly equivalent and should usually exceed the standard DT.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def test_boostedDT():\n",
    "\n",
    "  # load the data set\n",
    "  sklearn_dataset = datasets.load_breast_cancer()\n",
    "  # convert to pandas df\n",
    "  df = pd.DataFrame(sklearn_dataset.data,columns=sklearn_dataset.feature_names)\n",
    "  df['CLASS'] = pd.Series(sklearn_dataset.target)\n",
    "  df.head()\n",
    "\n",
    "  # split randomly into training/testing\n",
    "  train, test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "  # Split into X,y matrices\n",
    "  X_train = train.drop(['CLASS'], axis=1)\n",
    "  y_train = train['CLASS']\n",
    "  X_test = test.drop(['CLASS'], axis=1)\n",
    "  y_test = test['CLASS']\n",
    "\n",
    "\n",
    "  # train the decision tree\n",
    "  modelDT = DecisionTreeClassifier()\n",
    "  modelDT.fit(X_train, y_train)\n",
    "\n",
    "  # train the boosted DT\n",
    "  modelBoostedDT = BoostedDT(numBoostingIters=100, maxTreeDepth=2)\n",
    "  modelBoostedDT.fit(X_train, y_train)\n",
    "\n",
    "  # train sklearn's implementation of Adaboost\n",
    "  modelSKBoostedDT = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100)\n",
    "  modelSKBoostedDT.fit(X_train, y_train)\n",
    "\n",
    "  # output predictions on the test data\n",
    "  ypred_DT = modelDT.predict(X_test)\n",
    "  ypred_BoostedDT = modelBoostedDT.predict(X_test)\n",
    "  ypred_SKBoostedDT = modelSKBoostedDT.predict(X_test)\n",
    "\n",
    "  # compute the training accuracy of the model\n",
    "  accuracy_DT = accuracy_score(y_test, ypred_DT)\n",
    "  accuracy_BoostedDT = accuracy_score(y_test, ypred_BoostedDT)\n",
    "  accuracy_SKBoostedDT = accuracy_score(y_test, ypred_SKBoostedDT)\n",
    "\n",
    "  print(\"Decision Tree Accuracy = \"+str(accuracy_DT))\n",
    "  print(\"My Boosted Decision Tree Accuracy = \"+str(accuracy_BoostedDT))\n",
    "  print(\"Sklearn's Boosted Decision Tree Accuracy = \"+str(accuracy_SKBoostedDT))\n",
    "  print()\n",
    "  print(\"Note that due to randomization, your boostedDT might not always have the \")\n",
    "  print(\"exact same accuracy as Sklearn's boostedDT.  But, on repeated runs, they \")\n",
    "  print(\"should be roughly equivalent and should usually exceed the standard DT.\")\n",
    "\n",
    "test_boostedDT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenging Problem: Chocolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dropMissingColumn(inputDf,missing_thresh):\n",
    "    outputDf = inputDf.copy()\n",
    "    missDf = getMissingRatio(outputDf)\n",
    "    missIdx = missDf['MissingPercent'] >= missing_thresh\n",
    "    missFeat = missDf.loc[missIdx,'Feature']\n",
    "    outputDf = outputDf.drop(missFeat,axis=1)\n",
    "    return outputDf\n",
    "\n",
    "def fillMissingColumnSpecify(numericalDf,categoryDf):\n",
    "    numericalDf = numericalDf.fillna(numericalDf.mean())\n",
    "    categoryDf = categoryDf.fillna(categoryDf.mode().iloc[0]).astype('O')\n",
    "    categoryDf = pd.get_dummies(categoryDf)\n",
    "    outputDf = pd.concat([numericalDf,categoryDf],axis=1)\n",
    "    return outputDf\n",
    "\n",
    "def getMissingRatio(inputDf):\n",
    "    outSeries = inputDf.isna().mean()                 # calculate missing ratios\n",
    "    outDf = pd.DataFrame({'Feature':outSeries.index,\\\n",
    "                          'MissingPercent':outSeries.values})\n",
    "    return outDf\n",
    "\n",
    "def sortData(X,y,id_feature):\n",
    "    X = X.drop_duplicates(subset=id_feature)\n",
    "    y = y.drop_duplicates(subset=id_feature)\n",
    "    X, y = X.sort_values(by=[id_feature]), y.sort_values(by=[id_feature])\n",
    "    X_ind = X[id_feature].to_numpy()\n",
    "    y_ind = y[id_feature].to_numpy()\n",
    "    ind = np.intersect1d(X_ind,y_ind)\n",
    "    X = X.loc[X[id_feature].isin(ind)]\n",
    "    y = y.loc[y[id_feature].isin(ind)]\n",
    "    df_index = pd.Series(np.arange(X.shape[0]))\n",
    "    X = X.set_index(df_index)\n",
    "    y = y.set_index(df_index)\n",
    "    return X, y\n",
    "\n",
    "def filterUnlabel(X_train,X_unlabel):\n",
    "    train_columns = X_train.columns.values\n",
    "    unlabel_columns = X_unlabel.columns.values\n",
    "    lack_columns = np.setdiff1d(train_columns, unlabel_columns)\n",
    "    redundancy_columns = np.setdiff1d(unlabel_columns, train_columns)\n",
    "    X_unlabel = X_unlabel.assign(**dict.fromkeys(lack_columns,0))\n",
    "    X_unlabel.drop(redundancy_columns,axis=1)\n",
    "    return X_unlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_chocolate():\n",
    "    X = pd.read_csv('ChocolatePipes_trainData.csv')\n",
    "    y = pd.read_csv('ChocolatePipes_trainLabels.csv')\n",
    "    X, y = sortData(X, y, 'id')\n",
    "    useless_feature = ['id', 'Date of entry', 'Country funded by', \n",
    "                       'oompa loomper', 'Region code', 'District code',\n",
    "                       'Chocolate consumers in town', \n",
    "                       'Does factory offer tours', 'Recorded by',\n",
    "                       'Oompa loompa management', 'Payment scheme',\n",
    "                       'management_group']\n",
    "    X = X.drop(useless_feature,axis=1)\n",
    "    \n",
    "    X = dropMissingColumn(X, 0.5)\n",
    "    categorical_feature = ['chocolate_quality', 'chocolate_quantity',\n",
    "                           'pipe_type', 'chocolate_source',\n",
    "                           'chocolate_source_class', 'Cocoa farm',\n",
    "                           'Official or Unofficial pipe', \n",
    "                           'Type of pump','management']\n",
    "    Xc = X[categorical_feature]\n",
    "    Xn = X.drop(categorical_feature,axis=1)\n",
    "    \n",
    "    X = fillMissingColumnSpecify(Xn, Xc)\n",
    "    y = y.drop(['id'],axis=1)\n",
    "    \n",
    "    X_grade = pd.read_csv('ChocolatePipes_gradingTestData.csv')\n",
    "    X_grade_id = X_grade['id']\n",
    "    X_grade = X_grade.drop(useless_feature,axis=1)\n",
    "    Xc_grade = X_grade[categorical_feature]\n",
    "    Xn_grade = X_grade.drop(categorical_feature,axis=1)\n",
    "    X_grade = fillMissingColumnSpecify(Xn_grade, Xc_grade)\n",
    "    X_grade = filterUnlabel(X, X_grade)\n",
    "    \n",
    "    X_leader = pd.read_csv('ChocolatePipes_leaderboardTestData.csv')\n",
    "    X_leader_id = X_leader['id']\n",
    "    X_leader = X_leader.drop(useless_feature,axis=1)\n",
    "    Xc_leader = X_leader[categorical_feature]\n",
    "    Xn_leader = X_leader.drop(categorical_feature,axis=1)\n",
    "    X_leader = fillMissingColumnSpecify(Xn_leader, Xc_leader)\n",
    "    X_leader = filterUnlabel(X, X_leader)\n",
    "    \n",
    "    # BoostedDT\n",
    "    df = pd.concat([X,y],axis=1)\n",
    "    train, test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "    X_train = train.drop(['label'], axis=1)\n",
    "    y_train = train['label']\n",
    "    X_test = test.drop(['label'], axis=1)\n",
    "    y_test = test['label']\n",
    "    \n",
    "# =============================================================================\n",
    "    # tuning the best numBoostingIters and maxTreeDepth\n",
    "    max_train_accuracy = 0\n",
    "    max_test_accuracy = 0\n",
    "    max_train_iter = 0\n",
    "    max_test_iter = 0\n",
    "    for iter_num in range(25):\n",
    "        modelBoostedDT = BoostedDT(numBoostingIters=iter_num, maxTreeDepth=4)\n",
    "        modelBoostedDT.fit(X_train,y_train)\n",
    "        train_accuracy = (modelBoostedDT.predict(X_train).values.reshape(-1)\n",
    "                    ==y_train.values).sum()/len(y_train)\n",
    "        test_accuracy = (modelBoostedDT.predict(X_test).values.reshape(-1)\n",
    "                    ==y_test.values).sum()/len(y_test)\n",
    "        print(f'iteration: {iter_num}')\n",
    "        print(f'train: {train_accuracy}')\n",
    "        print(f'test: {test_accuracy}\\n')\n",
    "        if train_accuracy > max_train_accuracy:\n",
    "            max_train_accuracy = train_accuracy\n",
    "            max_train_iter = iter_num\n",
    "        if test_accuracy > max_test_accuracy:\n",
    "            max_test_accuracy = test_accuracy\n",
    "            max_test_iter = iter_num\n",
    "    print(f'max train performance: {max_train_accuracy}; iteration: {max_train_iter}')\n",
    "    print(f'max test performance: {max_test_accuracy}; iteration: {max_test_iter}')\n",
    "    modelBoostedDT = BoostedDT(numBoostingIters=max_test_iter, maxTreeDepth=4)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "    # Boosted Decision Tree\n",
    "    modelBoostedDT = BoostedDT(numBoostingIters=28, maxTreeDepth=18)\n",
    "    modelBoostedDT.fit(X_train,y_train)\n",
    "    train_accuracy_boostedDT = (modelBoostedDT.predict(X_train).values.reshape(-1)\n",
    "                     ==y_train.values).sum()/len(y_train)\n",
    "    test_accuracy_boostedDT = (modelBoostedDT.predict(X_test).values.reshape(-1)\n",
    "                     ==y_test.values).sum()/len(y_test)\n",
    "    print(f'train_accuracy_boostedDT: {train_accuracy_boostedDT}')\n",
    "    print(f'test_accuracy_boostedDT: {test_accuracy_boostedDT}')\n",
    "    \n",
    "    \n",
    "    modelBoostedDT.fit(X,y)\n",
    "    \n",
    "    y_grade_boost = modelBoostedDT.predict(X_grade)\n",
    "    output_grade_boost = pd.concat([X_grade_id, y_grade_boost], axis=1)\n",
    "    output_grade_boost.columns = ['id', 'label']\n",
    "    output_grade_boost.to_csv('predictions-grading-BoostedDT.csv',index=False)\n",
    "    \n",
    "    y_leader_boost = modelBoostedDT.predict(X_leader)\n",
    "    output_leader_boost = pd.concat([X_leader_id, y_leader_boost], axis=1)\n",
    "    output_leader_boost.columns = ['id', 'label']\n",
    "    output_leader_boost.to_csv('predictions-leaderboard-BoostedDT.csv',index=False)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "    # Preprocessing for SVM and Logistic Regression\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy().flatten()\n",
    "    \n",
    "    X_train = X_train.to_numpy()\n",
    "    y_train = y_train.to_numpy().flatten()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_test = y_test.to_numpy().flatten()\n",
    "    \n",
    "    X_grade = X_grade.to_numpy()\n",
    "    X_leader = X_leader.to_numpy()\n",
    "    \n",
    "    standardizer = StandardScaler()\n",
    "    Xstandardized = pd.DataFrame(standardizer.fit_transform(X))\n",
    "    Xstandardized_train = pd.DataFrame(standardizer.fit_transform(X_train))\n",
    "    Xstandardized_test = pd.DataFrame(standardizer.fit_transform(X_test))\n",
    "    Xstandardized_grade = pd.DataFrame(standardizer.fit_transform(X_grade))\n",
    "    Xstandardized_leader = pd.DataFrame(standardizer.fit_transform(X_leader))\n",
    "# =============================================================================\n",
    "    \n",
    "# =============================================================================\n",
    "#     # SVM\n",
    "    svm_clf = SVC(gamma='auto')\n",
    "    svm_clf.fit(Xstandardized_train, y_train)\n",
    "    train_accuracy_svm = (svm_clf.predict(Xstandardized_train)\n",
    "                            ==y_train).sum()/len(y_train)\n",
    "    test_accuracy_svm = (svm_clf.predict(Xstandardized_test)\n",
    "                            ==y_test).sum()/len(y_test)\n",
    "    print(f'train_accuracy_svm: {train_accuracy_svm}')\n",
    "    print(f'test_accuracy_svm: {test_accuracy_svm}')\n",
    "    \n",
    "    svm_clf.fit(Xstandardized, y)\n",
    "    \n",
    "    y_grade_svm = pd.DataFrame(svm_clf.predict(Xstandardized_grade))\n",
    "    output_grade_svm = pd.concat([X_grade_id, y_grade_svm], axis=1)\n",
    "    output_grade_svm.columns = ['id', 'label']\n",
    "    output_grade_svm.to_csv('predictions-grading-SVC.csv',index=False)\n",
    "    \n",
    "    y_leader_svm = pd.DataFrame(svm_clf.predict(Xstandardized_leader))\n",
    "    output_leader_svm = pd.concat([X_leader_id, y_leader_svm], axis=1)\n",
    "    output_leader_svm.columns = ['id', 'label']\n",
    "    output_leader_svm.to_csv('predictions-leaderboard-SVC.csv',index=False)\n",
    "# =============================================================================\n",
    "    \n",
    "# =============================================================================\n",
    "    # Logistic Regression\n",
    "    logistic_clf = LogisticRegression(random_state=42,max_iter=120)\n",
    "    logistic_clf.fit(Xstandardized_train, y_train)\n",
    "    train_accuracy_logistic = (logistic_clf.predict(Xstandardized_train)\n",
    "                                    ==y_train).sum()/len(y_train)\n",
    "    test_accuracy_logistic = (logistic_clf.predict(Xstandardized_test)\n",
    "                                    ==y_test).sum()/len(y_test)\n",
    "    print(f'train_accuracy_logistic: {train_accuracy_logistic}')\n",
    "    print(f'test_accuracy_logistic: {test_accuracy_logistic}')\n",
    "    \n",
    "    logistic_clf.fit(Xstandardized, y)\n",
    "    \n",
    "    y_grade_logistic = pd.DataFrame(logistic_clf.predict(Xstandardized_grade))\n",
    "    output_grade_logistic = pd.concat([X_grade_id, y_grade_logistic], axis=1)\n",
    "    output_grade_logistic.columns = ['id', 'label']\n",
    "    output_grade_logistic.to_csv('predictions-grading-best.csv',index=False)\n",
    "    \n",
    "    y_leader_logistic = pd.DataFrame(logistic_clf.predict(Xstandardized_leader))\n",
    "    output_leader_logistic = pd.concat([X_leader_id, y_leader_logistic], axis=1)\n",
    "    output_leader_logistic.columns = ['id', 'label']\n",
    "    output_leader_logistic.to_csv('predictions-leaderboard-best.csv',index=False)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chocolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4_skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
