\documentclass{article}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{tabularx}

\title{CIS 419/519: Homework 4}
\author{Jiatong Sun}
\date{02/29/2020}

\begin{document}
    \maketitle
    Although the solutions are entirely my own, I consulted with the following people and sources while working on this homework: $Jing$ $Zhao$\\
$https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols$\\
$https://tex.stackexchange.com/questions/122778/left-brace-including-several-lines-in-eqnarray$\\
$https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html$\\
$https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html$
    
    \section{Fitting an SVM by Hand}
        \begin{enumerate}[label=\alph*.]
            \item % a
            As is given in the problem:
            \begin{equation}
            	x1=0,\quad x2=\sqrt{2}
            \end{equation}
            and
            \begin{equation}
            	\phi(x)=[1,\sqrt{2},x^2]^T
            \end{equation}
            We know that
            \begin{equation}
            	\phi(x_1)=[1,0,0]^T
            	\quad
            	\phi(x_2)=[1,2,2]^T
            \end{equation}
            Since the optimal vector $\boldsymbol{w}$ is orthogonal to the dicision to the decision boundary, it is parallel to the vector connecting $\phi(x_1)$ and $\phi(x_2)$.\\\\
            Since
            \begin{equation}
            	\phi(x_2)-\phi(x_1)=
            	[1,2,2]^T-[1,0,0]^T=
            	[0,2,2]^T
            \end{equation}
            So $[0,2,2]^T$ is a vector that is parallel to the optimal vector $\boldsymbol{w}$.
            
            \item % b
            The margin is the distance between the two points in the 3D space.
            \begin{equation}
            	margin = ||\phi(x_2)-\phi(x_1)||
            	=\sqrt{(1-1)^2+(2-0)^2+(2-0)^2}
            	=2\sqrt{2}
            \end{equation}
            
            \item % c
            From the result of a, we can assume that
            \begin{equation}
            	\boldsymbol{w} = [0,2t,2t]^T
            \end{equation}
            So
            \begin{equation}
            	||\boldsymbol{w}||
            	=\sqrt{0^2+(2t)^2+(2t)^t}
            	=\sqrt{8t^2}=2\sqrt{2}t
            \end{equation}
            According to the relationship between $||w||$ and the length of the margin, we know that
            \begin{equation}
            	d=\frac{2}{||w||}
            	=\frac{1}{\sqrt{2}t}=2\sqrt{2}
            \end{equation}
            or
            \begin{equation}
            	t=\frac{1}{4}
            \end{equation}
            So
            \begin{equation}
            	\boldsymbol{w}
            	=[0,\frac{1}{2},\frac{1}{2}]^T
            \end{equation}
            
            \item % d
            According to SVM requirement,
            \begin{equation}
            	\begin{cases}
               		y_1(\boldsymbol{w}^T\phi(x_1)+w_0)
            		\geqslant 1\\
               		y_2(\boldsymbol{w}^T\phi(x_2)+w_0)
            		\geqslant 1
            	\end{cases}
            \end{equation}
            or
            \begin{equation}
            	\begin{cases}
               		-1\times(0+w_0)
            		\geqslant 1\\
               		1\times(2+w_0)
            		\geqslant 1
            	\end{cases}
            \end{equation}
            \begin{equation}
            	-1 \leqslant w_0 \leqslant -1
            \end{equation}
            So
            \begin{equation}
            	w_0 = -1
            \end{equation}
            
            \item % e
            \begin{equation}
            	h(x)=\boldsymbol{w}^T\phi(x)+w_0
            	=\frac{x^2}{2}+\frac{\sqrt{2}x}{2}-1
            \end{equation}
        \end{enumerate}    
        
        \section{Support Vector}
        There are two possibilies:\\\\
        1. Size of maximum margin increases, if a support vector determining the shortest margin is removed.\\\\
        2. Size of maximum margin stays the same, if the removed vector is not the one determining the shortest margin.
        
        \section{Challenge: Generalizing to Unseen Data}
        \textbf{Preprocessing}\\\\
        1. Sort the data in $X$ and $y$ in ascending direction and eliminate rows whose id only appears in $X$ or $y$. After this operation, $X$ and $y$ are aligned by an ascending id number.\\\\
        2. Drop useless features. I dropped the features below: ['id', 'Date of entry', 'Country funded by', 'oompa loomper', 'Region code', 'District code', 'Chocolate consumers in town', 'Does factory offer tours', 'Recorded by','Oompa loompa management', 'Payment scheme', 'management group'].\\\\
        3. Drop columns whose missing ratio is higher than 50\%.\\\\
        4. Define categorical features (even though some features look like numerical features, if they are defined as categorical in the pdf, they need to be converted to object dtypes).\\\\
        5. Fill missing data (mean for numerical and mode for categorical). Use OHE to process the categorical data.\\\\
        6. Drop the id column of label.\\\\
        7. For unlabeled data set, after step (2)-(5), we still need to add missing features and reduce redundant features with respect to the training data set or otherwise the data cannot be predicted.\\\\
        
        \noindent 
        \textbf{The Best Classifier}\\\\
        The best classifier I found is the Support Vector Machine (SVM). Before training the model, I standardized the training data set, which turns out to be a great way of increasing the accuracy. All the parameters are set as default in according to skicit. 
        
        \noindent
        \textbf{Results and Discussion}\\\\
        \begin{center}
		\begin{tabularx}{0.8\textwidth} { 
 			| >{\centering\arraybackslash}X 
  			| >{\centering\arraybackslash}X 
   			| >{\centering\arraybackslash}X | }
   			\hline
   			\multicolumn{3}{|c|}
   			{\textbf{Algorithms Comparison}}\\
 			\hline
 			& training accuracy & generalized accuracy \\
 			\hline
 			Boosted Decision Tree & 0.9999 & 0.7742\\
 			\hline
 			Support Vector Machine & 0.7628 & 0.7429\\
 			\hline
 			Logistic Regression & 0.7196 & 0.7189\\
 			\hline
		\end{tabularx}  
		\end{center} 
		According to the result table, we can notice that the Boosted Decision Tree has the best performance among all three. Its training accuracy is extremely high, but its generalized accuracy is not that high. This may be because of overfitting to some extent, but its generalized ability is still the best with respect to unseen data set, which is an advantage of the Boosted Dicision Tree. Also, During the test, the SVM works particularly slow, which demonstrates its slow converge speed. The Logistic Regression is quick but its performance is not brilliant.
       
\end{document}